<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>DEEPSYNTH â€” A Benchmark for Deep Information Synthesis</title>
  <meta name="description" content="DEEPSYNTH: 120 challenging tasks across 67 countries evaluating LLM agents on multi-source information synthesis. ICLR 2026." />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;500;700&family=Noto+Sans:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" />
  <style>
    :root { --max-w:900px; --text:#333; --text-light:#555; --text-dim:#888; --accent:#4a6cf7; --bg:#fff; --border:#e5e7eb; --surface:#f8f9fb; }
    *{margin:0;padding:0;box-sizing:border-box;} html{scroll-behavior:smooth;}
    body{font-family:'Noto Sans',sans-serif;color:var(--text);background:var(--bg);line-height:1.8;font-size:18px;}
    .container{max-width:var(--max-w);margin:0 auto;padding:0 24px;}

    /* Hero */
    .hero{text-align:center;padding:60px 24px 32px;}
    .hero-logo{width:80px;border-radius:10px;vertical-align:middle;}
    .venue-badge{display:inline-block;font-size:.8rem;font-weight:600;letter-spacing:.5px;color:var(--accent);border:1.5px solid var(--accent);padding:4px 16px;border-radius:20px;margin-bottom:20px;text-transform:uppercase;}
    .hero-title-row{display:flex;align-items:center;justify-content:center;gap:18px;margin-bottom:6px;}
    .hero h1{font-family:'Google Sans','Noto Sans',sans-serif;font-size:2.8rem;font-weight:700;color:#222;}
    .hero h2{font-family:'Google Sans','Noto Sans',sans-serif;font-size:1.4rem;font-weight:400;color:var(--text-light);margin-bottom:30px;}
    .hero-authors{font-size:1.1rem;color:var(--text-light);line-height:2.1;margin-bottom:6px;}
    .hero-authors a{color:var(--text);text-decoration:none;font-weight:500;}
    .hero-authors a:hover{color:var(--accent);}
    .hero-authors sup{color:var(--accent);font-weight:600;font-size:.7rem;}
    .hero-affil{font-size:.9rem;color:var(--text-dim);line-height:1.8;margin-bottom:30px;}
    .hero-affil sup{color:var(--accent);font-weight:600;}
    .btn-row{display:flex;flex-wrap:wrap;justify-content:center;gap:10px;margin-bottom:12px;}
    .btn{display:inline-flex;align-items:center;gap:6px;padding:10px 22px;border-radius:20px;font-size:.95rem;font-weight:600;text-decoration:none;border:1.5px solid var(--border);color:var(--text);background:var(--bg);transition:all .2s;}
    .btn:hover{border-color:var(--accent);color:var(--accent);}
    .btn i{font-size:.9em;}

    /* Sections */
    section{padding:40px 0;}
    .divider{border:none;border-top:1px solid var(--border);margin:0;}
    h3.section-title{font-family:'Google Sans','Noto Sans',sans-serif;font-size:1.8rem;font-weight:700;color:#222;text-align:center;margin-bottom:20px;}
    .prose{color:var(--text-light);font-size:1rem;line-height:1.8;text-align:justify;}
    .prose p+p{margin-top:14px;}
    .prose strong{color:var(--text);}

    /* Figures */
    .figure{margin:28px 0;border-radius:8px;overflow:hidden;border:1px solid var(--border);}
    .figure img{width:100%;display:block;height:auto;}
    .figure-cap{padding:10px 16px;font-size:.82rem;color:var(--text-dim);background:var(--surface);text-align:center;}
    .figure-cap strong{color:var(--text-light);}
    .fig-row{display:grid;grid-template-columns:1fr 1fr;gap:14px;margin:24px 0;}
    .fig-row .figure{margin:0;}

    /* Tables */
    .table-wrap{overflow-x:auto;-webkit-overflow-scrolling:touch;margin:20px 0;border:1px solid var(--border);border-radius:8px;}
    table{width:100%;border-collapse:collapse;font-size:.82rem;}
    thead{background:var(--surface);}
    th{padding:9px 10px;text-align:left;font-weight:600;border-bottom:2px solid var(--border);white-space:nowrap;}
    td{padding:8px 10px;border-bottom:1px solid #f0f1f3;white-space:nowrap;color:var(--text-light);}
    tbody tr:hover{background:#fafbff;}
    .row-best td{color:var(--accent);font-weight:600;}
    .row-sep td{font-size:.7rem;font-weight:600;text-transform:uppercase;letter-spacing:1px;color:var(--text-dim);background:var(--surface);padding:8px 10px 4px;border-bottom:1px solid var(--border);}
    td.n,th.n{text-align:right;font-family:'JetBrains Mono',monospace;font-size:.8rem;}

    /* Callout */
    .callout{background:var(--surface);border-left:3px solid var(--accent);border-radius:0 8px 8px 0;padding:16px 20px;margin:20px 0;font-size:.9rem;color:var(--text-light);line-height:1.7;}
    .callout.red{border-left-color:#e74c3c;} .callout.green{border-left-color:#27ae60;}
    .callout strong{color:var(--text);}

    /* Criteria */
    .criteria-grid{display:grid;grid-template-columns:1fr 1fr;gap:12px;margin:20px 0;}
    .crit{background:var(--surface);border-radius:8px;padding:16px 18px;border-left:3px solid var(--accent);}
    .crit h4{font-size:.9rem;font-weight:600;color:var(--text);margin-bottom:4px;}
    .crit p{font-size:.82rem;color:var(--text-dim);line-height:1.6;}
    .crit.full{grid-column:1/-1;}

    /* Cards */
    .card-row{display:grid;grid-template-columns:repeat(2,1fr);gap:12px;margin:20px 0;}
    .card{border:1px solid var(--border);border-radius:8px;padding:20px;text-align:center;transition:border-color .2s;}
    .card:hover{border-color:var(--accent);}
    .card h4{font-size:.95rem;margin-bottom:6px;}
    .card p{font-size:.82rem;color:var(--text-dim);margin-bottom:12px;}
    .card .btn{font-size:.78rem;padding:6px 14px;}

    /* Error grid */
    .err-grid{display:grid;grid-template-columns:1fr 1fr;gap:10px;margin:16px 0;}
    .err-item{display:flex;align-items:center;gap:10px;background:var(--surface);border-radius:8px;padding:12px 14px;}
    .err-num{font-family:'JetBrains Mono',monospace;font-size:1.3rem;font-weight:700;color:var(--text);min-width:28px;flex-shrink:0;}
    .err-label{font-size:.8rem;color:var(--text-dim);}

    /* BibTeX */
    .bib-block{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:16px;margin-top:16px;position:relative;overflow-x:auto;}
    .bib-block pre{font-family:'JetBrains Mono',monospace;font-size:.73rem;color:var(--text-light);white-space:pre-wrap;word-wrap:break-word;line-height:1.6;}
    .copy-btn{position:absolute;top:10px;right:10px;background:var(--bg);border:1px solid var(--border);color:var(--text-dim);padding:4px 10px;border-radius:4px;font-size:.7rem;font-family:'JetBrains Mono',monospace;cursor:pointer;}
    .copy-btn:hover{color:var(--accent);border-color:var(--accent);}

    footer{text-align:center;padding:24px;font-size:.78rem;color:var(--text-dim);border-top:1px solid var(--border);margin-top:20px;}
    footer a{color:var(--accent);}

    /* ========== TABLET (â‰¤768px) ========== */
    @media(max-width:768px){
      body{font-size:16px;}
      .hero{padding:40px 16px 24px;}
      .hero h1{font-size:2.1rem;}
      .hero h2{font-size:1.15rem;margin-bottom:22px;}
      .hero-title-row{gap:12px;}
      .hero-logo{width:64px;}
      .hero-authors{font-size:.95rem;line-height:1.9;}
      .hero-affil{font-size:.82rem;margin-bottom:22px;}
      .container{padding:0 16px;}
      h3.section-title{font-size:1.4rem;margin-bottom:16px;}
      .prose{font-size:.92rem;text-align:left;}
      section{padding:30px 0;}
      .btn{padding:9px 18px;font-size:.88rem;}
      .criteria-grid{grid-template-columns:1fr;}
      .crit.full{grid-column:auto;}
      .fig-row{grid-template-columns:1fr;}
      .card-row{grid-template-columns:1fr;}
      .err-grid{grid-template-columns:1fr;}
      .callout{padding:14px 16px;font-size:.86rem;}
      table{font-size:.78rem;}
      td.n,th.n{font-size:.74rem;padding:7px 8px;}
      th{padding:8px 8px;}
      td{padding:7px 8px;}
    }

    /* ========== PHONE (â‰¤480px) ========== */
    @media(max-width:480px){
      .hero{padding:28px 12px 18px;}
      .hero-title-row{flex-direction:column;gap:8px;}
      .hero h1{font-size:1.7rem;}
      .hero h2{font-size:1rem;margin-bottom:18px;}
      .hero-logo{width:56px;}
      .hero-authors{font-size:.84rem;line-height:1.75;}
      .hero-affil{font-size:.74rem;margin-bottom:18px;}
      .venue-badge{font-size:.7rem;padding:3px 12px;}
      .container{padding:0 12px;}
      h3.section-title{font-size:1.15rem;margin-bottom:12px;}
      .prose{font-size:.84rem;line-height:1.7;}
      .btn-row{gap:6px;}
      .btn{padding:7px 14px;font-size:.8rem;border-radius:16px;}
      .crit{padding:12px 14px;}
      .crit h4{font-size:.84rem;}
      .crit p{font-size:.76rem;}
      .figure{margin:18px 0;}
      .figure-cap{font-size:.74rem;padding:8px 12px;}
      .callout{font-size:.82rem;padding:12px 14px;}
      .err-item{padding:10px 12px;}
      .err-num{font-size:1.1rem;}
      .err-label{font-size:.74rem;}
      .card{padding:14px;}
      .card h4{font-size:.88rem;}
      .card p{font-size:.76rem;margin-bottom:10px;}
      .bib-block{padding:12px;}
      .bib-block pre{font-size:.63rem;}
      .copy-btn{font-size:.63rem;padding:3px 8px;top:8px;right:8px;}
      footer{padding:18px 12px;font-size:.72rem;}
    }
  </style>
</head>
<body>

<!-- ===== HERO ===== -->
<div class="hero">
  
  <div class="hero-title-row">
    <img src="assets/deepsynth_octopus_logo.png" alt="DEEPSYNTH Logo" class="hero-logo" />
    <h1>DEEPSYNTH</h1>
  </div>
  <h2>A Benchmark for Deep Information Synthesis</h2>

  <div class="hero-authors">
    <a href="https://debjitpaul.github.io/">Debjit Paul</a><sup>1</sup>,
    <a href="#">Daniel Murphy</a><sup>3</sup>,
    <a href="#">Milan Gritta</a><sup>1</sup>,
    <a href="#">Ronald Cardenas</a><sup>1</sup>,
    <a href="#">Victor Prokhorov</a><sup>1</sup>,
    <a href="#">Jun Wang</a><sup>2</sup>,
    <a href="#">Gerasimos Lampouras</a><sup>1</sup>
  </div>
  <div class="hero-authors" style="margin-bottom:4px;">
    <em>Dataset Contributors:</em>
    <a href="#">Lena Sophia Bolliger</a><sup>6</sup>,
    <a href="#">Aysim Toker</a><sup>1</sup>,
    <a href="#">Roy Miles</a><sup>1</sup>,
    <a href="#">Andreea-Maria Oncescu</a><sup>1</sup>,
    <a href="#">Jasivan Alex Sivakumar</a><sup>4</sup>,
    <a href="#">Philipp Borchert</a><sup>1</sup>,
    <a href="#">Ismail Elezi</a><sup>1</sup>,
    <a href="#">Meiru Zhang</a><sup>5</sup>,
    <a href="#">Ka Yiu Lee</a><sup>1</sup>,
    <a href="#">Guchun Zhang</a><sup>1</sup>
  </div>
  <div class="hero-affil">
    <sup>1</sup>Huawei Noah's Ark Lab &ensp;
    <sup>2</sup>UCL Centre for AI &ensp;
    <sup>3</sup>Imperial College London &ensp;
    <sup>4</sup>University of Sheffield &ensp;
    <sup>5</sup>University of Cambridge &ensp;
    <sup>6</sup>University of Zurich
  </div>
  <span class="venue-badge"><i class="fas fa-award"></i> &nbsp;Published at ICLR 2026</span>
  <div class="btn-row">
    <a href="https://openreview.net/pdf?id=0Dhpt9aY3n" class="btn" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
    <a href="#" class="btn"><i class="fab fa-github"></i> Code</a>
    <a href="#data" class="btn"><i class="fas fa-database"></i> Data</a>
    <a href="#citation" class="btn"><i class="fas fa-quote-left"></i> Cite</a>
  </div>
</div>

<div class="container">

<hr class="divider" />

<!-- ===== ABSTRACT ===== -->
<section>
  <h3 class="section-title">Abstract</h3>
  <div class="prose">
    <p>Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval.</p>
    <p>We introduce <strong>DEEPSYNTH</strong>, a novel benchmark of <strong>120 tasks</strong> across <strong>7 domains</strong> and <strong>67 countries</strong>, designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning. When evaluated on DEEPSYNTH, <strong>9 state-of-the-art LLMs and deep research agents achieve a maximum F1 of only 8.97</strong>. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces.</p>
  </div>
</section>

<hr class="divider" />

<!-- ===== HOW IT WORKS â€” Figure 1 ===== -->
<section>
  <h3 class="section-title">How DEEPSYNTH Works</h3>
  <div class="figure">
    <img src="assets/deepsynth_figure1.gif" alt="DEEPSYNTH pipeline animation" />
    <div class="figure-cap"><strong>Figure 1.</strong> A sample task illustrating the multi-step agent pipeline: web search â†’ browse multiple sources â†’ extract & filter data â†’ reason â†’ generate structured JSON answer.</div>
  </div>
</section>

<hr class="divider" />

<!-- ===== BENCHMARK: CRITERIA + DATA COLLECTION ===== -->
<section>
  <h3 class="section-title">The DEEPSYNTH Benchmark</h3>
  <div class="prose">
    <p>DEEPSYNTH evaluates agents on their ability to navigate multiple websites, extract information from both structured and unstructured sources, and reason effectively to produce correct solutions. Each task yields a concise JSON output enabling straightforward verification. The design of DEEPSYNTH tasks is driven by five criteria:</p>
  </div>

  <div class="criteria-grid">
    <div class="crit"><h4>a) Multi-source Synthesis</h4><p>Tasks require identifying connections across multiple data sources and combining information to produce a coherent solution.</p></div>
    <div class="crit"><h4>b) Real-World Inspired</h4><p>Tasks are designed so that insights would conceivably shape decisions of policy makers, travel agents, political scientists, etc.</p></div>
    <div class="crit"><h4>c) Verifiable Answers</h4><p>Each task has a closed-form JSON answer that can be automatically verified and is stable over time for reproducible evaluation.</p></div>
    <div class="crit"><h4>d) Diversity</h4><p>Tasks span 67 countries and 7 domains with temporal analyses, comparative evaluations, and relational reasoning.</p></div>
    <div class="crit full"><h4>e) Robust Against Memorisation</h4><p>Gold-standard answers are intentionally non-retrievable through verbatim lookup, compelling agents to plan and perform multi-step reasoning to derive the correct output.</p></div>
  </div>

  <!-- Data Collection Pipeline Figure -->
  <div class="prose" style="margin-top:24px;">
    <p><strong>Data Collection Pipeline.</strong> Building DEEPSYNTH involved four key stages: (a) identifying data sources, (b) gathering hypotheses, (c) validating hypotheses through analysis, and (d) formulating tasks with intermediate steps. 16 human experts (81.25% PhD holders) proposed 223 data sources across 7 domains. All tasks underwent independent double-annotation; only tasks with agreement were retained, yielding the final 120 tasks.</p>
  </div>

  <div class="figure">
    <img src="assets/data_collection_example.png" alt="Data collection pipeline for DEEPSYNTH" />
    <div class="figure-cap"><strong>Figure 2.</strong> Overview of the four-stage data collection process: data source identification, hypothesis gathering, hypothesis validation, and task formulation.</div>
  </div>

  <!-- Tool Use Distribution Figure -->
  <div class="prose" style="margin-top:20px;">
    <p><strong>Required Capabilities.</strong> Web search and browsing are needed for 100% of tasks, while 45% require diverse filetype reading, 43% need code execution, and 3% involve multi-modal inputs.</p>
  </div>

  <div class="figure">
    <img src="assets/tool_use_distribution.png" alt="Tool capabilities required for DEEPSYNTH tasks" />
    <div class="figure-cap"><strong>Figure 3.</strong> Percentage of tasks per capability required to solve DEEPSYNTH.</div>
  </div>
</section>

<hr class="divider" />

<!-- ===== MAIN RESULTS ===== -->
<section>
  <h3 class="section-title">Main Results</h3>
  <div class="table-wrap">
    <table>
      <thead><tr><th>Model</th><th class="n">F1</th><th class="n">Prec.</th><th class="n">Recall</th><th class="n">EM</th><th class="n">LLM Judge</th></tr></thead>
      <tbody>
        <tr class="row-sep"><td colspan="6">LLM Baselines</td></tr>
        <tr><td>o4-mini</td><td class="n">3.05</td><td class="n">2.33</td><td class="n">4.39</td><td class="n">0.0</td><td class="n">0.0</td></tr>
        <tr><td>GPT-4.1</td><td class="n">3.46</td><td class="n">2.86</td><td class="n">4.39</td><td class="n">0.0</td><td class="n">0.0</td></tr>
        <tr><td>o3</td><td class="n">3.29</td><td class="n">2.85</td><td class="n">3.90</td><td class="n">0.0</td><td class="n">0.0</td></tr>
        <tr><td>GPT-5.1</td><td class="n">3.83</td><td class="n">2.98</td><td class="n">5.37</td><td class="n">0.0</td><td class="n">0.0</td></tr>
        <tr><td>Gemini-Pro-2.5</td><td class="n">6.25</td><td class="n">4.71</td><td class="n">9.27</td><td class="n">0.0</td><td class="n">5.0</td></tr>
        <tr><td>GPT-5.2-Pro</td><td class="n">8.70</td><td class="n">8.45</td><td class="n">8.96</td><td class="n">6.25</td><td class="n">3.3</td></tr>
        <tr><td>DeepSeek-R1-Chat</td><td class="n">3.23</td><td class="n">2.75</td><td class="n">3.90</td><td class="n">1.67</td><td class="n">2.5</td></tr>
        <tr><td>DeepSeek-R1-Reasoner</td><td class="n">2.80</td><td class="n">2.73</td><td class="n">2.87</td><td class="n">2.50</td><td class="n">6.67</td></tr>
        <tr class="row-sep"><td colspan="6">Framework-based Agents</td></tr>
        <tr class="row-best"><td>o3-deep-research</td><td class="n">8.97</td><td class="n">7.73</td><td class="n">10.69</td><td class="n">2.50</td><td class="n">17.5</td></tr>
        <tr><td>Smolagent (GPT-4.1)</td><td class="n">3.75</td><td class="n">3.27</td><td class="n">4.39</td><td class="n">2.50</td><td class="n">7.5</td></tr>
        <tr><td>Smolagent (GPT-5)</td><td class="n">6.42</td><td class="n">6.34</td><td class="n">6.50</td><td class="n">1.67</td><td class="n">2.5</td></tr>
        <tr><td>OWL (GPT-4.1)</td><td class="n">5.41</td><td class="n">4.62</td><td class="n">6.52</td><td class="n">1.67</td><td class="n">12.5</td></tr>
      </tbody>
    </table>
  </div>

  <div class="callout red">
    <strong>Key finding:</strong> Under strict exact-match, almost all LLM baselines score zero. The best agent (o3-deep-research) solves only 3 of 120 tasks exactly, confirming that parametric knowledge alone is insufficient.
  </div>
</section>

<hr class="divider" />

<!-- ===== DEEPSYNTH-Dev + Best@N ===== -->
<section>
  <h3 class="section-title">DEEPSYNTH-Dev Results</h3>
  <div class="prose">
    <p>We evaluate on the <strong>DEEPSYNTH-Dev</strong> (Lite) subset. Among standalone LLMs, GPT-5.2 achieves the highest F1 (15.6), while Gemini-Pro-3 leads on LLM-Judge (15.0). Among agents, o3-deep-research attains the highest LLM-Judge score (20.0), reinforcing that tool augmentation benefits synthesis-heavy tasks.</p>
  </div>

  <div class="fig-row">
    <div class="figure">
      <img src="assets/deepsynth_dev_results_v2.png" alt="DEEPSYNTH-Dev Pass@1 results" />
      <div class="figure-cap"><strong>Figure 4a.</strong> Pass@1 performance on DEEPSYNTH-Dev.</div>
    </div>
    <div class="figure">
      <img src="assets/best_at_n_with_sc.png" alt="Best@N and Self-Consistency results" />
      <div class="figure-cap"><strong>Figure 4b.</strong> Best@N and Self-Consistency@5 on DEEPSYNTH-Dev.</div>
    </div>
  </div>

  <div class="callout">
    <strong>Best@N vs Self-Consistency:</strong> Under Best@5, Smolagents reaches 25.0% LLM-Judge accuracy vs. only 5.0% with majority voting â€” current agents exhibit high output variance where occasional runs succeed but models lack reliability.
  </div>
</section>

<hr class="divider" />

<!-- ===== ANALYSIS ===== -->
<section>
  <h3 class="section-title">Analysis</h3>

  <div class="prose"><p><strong>Error Propagation.</strong> Evaluating intermediate step accuracy on 40 tasks reveals steep decay: retrieval steps achieve 2â€“12% F1, computation steps collapse to near zero. When a step fails, the next step also fails 91â€“100% of the time.</p></div>

  <div class="table-wrap">
    <table>
      <thead><tr><th>Step</th><th class="n">DeepSeek-R1</th><th class="n">GPT-4.1</th><th class="n">GPT-5.2</th><th class="n">Prop. (%)</th></tr></thead>
      <tbody>
        <tr><td>Step 1 </td><td class="n">11.2</td><td class="n">10.0</td><td class="n">4.1</td><td class="n">â€”</td></tr>
        <tr><td>Step 2 </td><td class="n">12.4</td><td class="n">9.8</td><td class="n">2.6</td><td class="n">97.0</td></tr>
        <tr><td>Step 3 </td><td class="n">3.9</td><td class="n">3.3</td><td class="n">0.5</td><td class="n">100.0</td></tr>
        <tr><td>Step 4 </td><td class="n">1.4</td><td class="n">2.4</td><td class="n">0.0</td><td class="n">100.0</td></tr>
        <tr><td>Step 5+ </td><td class="n">0.0â€“0.2</td><td class="n">0.0</td><td class="n">0.0</td><td class="n">100.0</td></tr>
        <tr><td><strong>Final Answer</strong></td><td class="n"><strong>20.1</strong></td><td class="n"><strong>18.5</strong></td><td class="n"><strong>16.7</strong></td><td class="n">â€”</td></tr>
      </tbody>
    </table>
  </div>

  <div class="prose" style="margin-top:20px;"><p><strong>Error Types.</strong> Manual analysis of 32 errors from OWL (GPT-4.1):</p></div>
  <div class="err-grid">
    <div class="err-item"><div class="err-num">16</div><div class="err-label">Synthesis â€” wrong conclusions despite correct data</div></div>
    <div class="err-item"><div class="err-num">15</div><div class="err-label">Navigation â€” failed to locate the correct source</div></div>
    <div class="err-item"><div class="err-num">4</div><div class="err-label">No answer produced</div></div>
    <div class="err-item"><div class="err-num">4</div><div class="err-label">Technical / tool failures</div></div>
  </div>

  <div class="prose" style="margin-top:20px;"><p><strong>Geographic Bias.</strong> All models score F1 0.0 on Africa-related tasks (8.3% of benchmark). Performance varies sharply by region:</p></div>
  <div class="table-wrap">
    <table>
      <thead><tr><th>Region</th><th class="n">%</th><th class="n">GPT-4.1</th><th class="n">o3-deep-res.</th><th class="n">Gemini-2.5</th><th class="n">Smolagents</th></tr></thead>
      <tbody>
        <tr><td>Africa</td><td class="n">8.3</td><td class="n">0.0</td><td class="n">0.0</td><td class="n">0.0</td><td class="n">0.0</td></tr>
        <tr><td>North America</td><td class="n">11.7</td><td class="n">4.65</td><td class="n">8.00</td><td class="n">12.00</td><td class="n">8.33</td></tr>
        <tr><td>South America</td><td class="n">5.0</td><td class="n">0.0</td><td class="n">25.00</td><td class="n">0.0</td><td class="n">0.0</td></tr>
        <tr><td>Asia</td><td class="n">29.2</td><td class="n">3.36</td><td class="n">12.70</td><td class="n">6.50</td><td class="n">11.88</td></tr>
        <tr><td>Europe</td><td class="n">38.3</td><td class="n">3.45</td><td class="n">10.83</td><td class="n">4.91</td><td class="n">5.28</td></tr>
        <tr><td>Oceania</td><td class="n">10.8</td><td class="n">8.96</td><td class="n">14.43</td><td class="n">6.67</td><td class="n">24.00</td></tr>
      </tbody>
    </table>
  </div>

  <div class="callout green">
    <strong>Planning is the bottleneck.</strong> Providing ground-truth intermediate steps (without answers) boosts GPT-4.1 from 3.46 â†’ 9.36 F1 and Smolagent from 3.75 â†’ 10.50 F1 â€” current agents lack effective planning, not reasoning ability.
  </div>
</section>

<hr class="divider" />

<!-- ===== DATASET & CODE ===== -->
<section id="data">
  <h3 class="section-title">Dataset &amp; Code</h3>
  <div class="prose" style="text-align:center;"><p>DEEPSYNTH is released in two splits: <strong>Dev (Lite)</strong> for prototyping and <strong>Test</strong> for evaluation.</p></div>
  <div class="card-row">
    <div class="card"><h4>ðŸ“„ Paper</h4><p>Full paper on OpenReview</p><a href="https://openreview.net/pdf?id=0Dhpt9aY3n" class="btn" target="_blank"><i class="fas fa-external-link-alt"></i> OpenReview</a></div>
    <div class="card"><h4>ðŸ’» Code</h4><p>Eval scripts &amp; baselines</p><a href="#" class="btn"><i class="fab fa-github"></i> GitHub</a></div>
    <div class="card"><h4>ðŸ§ª Dev (Lite)</h4><p>Quick iteration split</p><a href="#" class="btn"><i class="fas fa-download"></i> Coming Soon</a></div>
    <div class="card"><h4>ðŸ“Š Test Set</h4><p>Full 120-task benchmark</p><a href="#" class="btn"><i class="fas fa-download"></i> Coming Soon</a></div>
  </div>
</section>

<hr class="divider" />

<!-- ===== CITATION ===== -->
<section id="citation">
  <h3 class="section-title">BibTeX</h3>
  <div class="bib-block">
    <button class="copy-btn" onclick="copyBib()" id="copyBtn"><i class="far fa-copy"></i> Copy</button>
    <pre id="bibTex">@inproceedings{paul2026deepsynth,
  title     = {{DEEPSYNTH}: A Benchmark for Deep Information Synthesis},
  author    = {Debjit Paul and Daniel Murphy and Milan Gritta and Ronald Cardenas and Victor Prokhorov 
               and Lena Sophia Bolliger and Aysim Toker and Roy Miles and Andreea-Maria Oncescu and
               Jasivan Alex Sivakumar and Philipp Borchert and Ismail Elezi and Meiru Zhang and 
               Ka Yiu Lee and Guchun Zhang and Jun Wang and Gerasimos Lampouras},
  booktitle = {The Fourteenth International Conference on
               Learning Representations (ICLR)},
  year      = {2026},
  url       = {https://openreview.net/forum?id=0Dhpt9aY3n}
}</pre>
  </div>
</section>

</div>

<footer>
  DEEPSYNTH &copy; 2026. Built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">Academic Project Page Template</a>.
</footer>

<script>
function copyBib(){navigator.clipboard.writeText(document.getElementById('bibTex').textContent).then(()=>{const b=document.getElementById('copyBtn');b.innerHTML='<i class="fas fa-check"></i> Copied!';setTimeout(()=>b.innerHTML='<i class="far fa-copy"></i> Copy',2000);});}
</script>
</body>
</html>
